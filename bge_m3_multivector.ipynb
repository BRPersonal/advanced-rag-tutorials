{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGE-M3 Multi-Vector Hybrid Retrieval for RAG\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the **most advanced retrieval tutorial** in this series! This notebook demonstrates state-of-the-art multi-vector retrieval using **BGE-M3**, which combines three different retrieval methods in a single model.\n",
    "\n",
    "### What is BGE-M3?\n",
    "\n",
    "**BGE-M3** (released January 2024 by BAAI) is a breakthrough embedding model where **M3** stands for:\n",
    "\n",
    "1. **Multi-linguality**: Supports 100+ languages\n",
    "2. **Multi-granularities**: Handles up to 8,192 tokens\n",
    "3. **Multi-Functionality**: Unifies THREE retrieval methods in one model\n",
    "\n",
    "### The Three Retrieval Modes\n",
    "\n",
    "BGE-M3 can simultaneously perform:\n",
    "\n",
    "#### 1. ðŸŽ¯ Dense Retrieval\n",
    "- Traditional semantic embeddings (1024 dimensions)\n",
    "- One vector per document\n",
    "- Fast and good for semantic similarity\n",
    "- Uses [CLS] token from BERT\n",
    "\n",
    "#### 2. ðŸ” Sparse Retrieval\n",
    "- Learned sparse vectors (like SPLADE)\n",
    "- Better than traditional BM25 (neural vs statistical)\n",
    "- Only non-zero weights for important terms\n",
    "- Vocabulary-sized with mostly zeros\n",
    "\n",
    "#### 3. ðŸ§  Multi-Vector (ColBERT)\n",
    "- Token-level late interaction\n",
    "- One vector per token in document\n",
    "- Highest quality but most expensive\n",
    "- Best used for reranking, not initial retrieval\n",
    "\n",
    "### IBM Research: Why This Matters\n",
    "\n",
    "IBM's 2024 **\"Blended RAG\"** paper demonstrated that:\n",
    "\n",
    "- âœ… **3-way retrieval is optimal** for RAG systems\n",
    "- âœ… Combining BM25 + dense + sparse achieved **NDCG@10 of 0.87** (8.2% improvement)\n",
    "- âœ… Adding ColBERT reranker yields **even better results**\n",
    "\n",
    "> *\"Three-way retrieval using all approaches is the optimal option for RAG\"* - IBM Research\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "\n",
    "- âœ… BGE-M3's three retrieval modes and when to use each\n",
    "- âœ… How ColBERT's late interaction works (token-level matching)\n",
    "- âœ… Multi-stage retrieval: candidates â†’ reranking\n",
    "- âœ… How to combine methods with RRF (Reciprocal Rank Fusion)\n",
    "- âœ… Production architecture for optimal quality\n",
    "- âœ… Trade-offs: quality vs speed vs storage\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- âœ… Milvus server running on `localhost:19530`\n",
    "- âœ… Python 3.8+\n",
    "- âœ… GPU recommended (but CPU works)\n",
    "- âœ… Basic understanding of embeddings and vector search\n",
    "\n",
    "Let's dive into state-of-the-art retrieval! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: Setup and Installation\n",
    "\n",
    "## Required Packages\n",
    "\n",
    "We'll install:\n",
    "- **FlagEmbedding**: BGE-M3 model implementation\n",
    "- **pymilvus[model]**: Milvus vector database with model support\n",
    "- **datasets**: Hugging Face datasets (climate_fever)\n",
    "- **matplotlib, seaborn**: Visualizations\n",
    "- **numpy**: Numerical computations for ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 1\n",
    "# Install core packages\n",
    "# !uv pip install FlagEmbedding  # BGE-M3 model\n",
    "# !uv pip install pymilvus[model]  # Milvus vector database\n",
    "# !uv pip install datasets  # Hugging Face datasets\n",
    "# !uv pip install matplotlib seaborn  # Visualizations\n",
    "# !uv pip install numpy torch  # Numerical computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 2\n",
    "# ============================================================\n",
    "# Import Libraries\n",
    "# ============================================================\n",
    "\n",
    "# BGE-M3 model\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "# Milvus vector database\n",
    "from pymilvus import (\n",
    "    MilvusClient,           # High-level client\n",
    "    connections,            # Connection management\n",
    "    FieldSchema,            # Schema field definition\n",
    "    CollectionSchema,       # Collection schema\n",
    "    DataType,               # Data types\n",
    "    Collection,             # Collection operations\n",
    "    AnnSearchRequest,       # Search request for hybrid search\n",
    "    RRFRanker,              # Reciprocal Rank Fusion\n",
    "    utility                 # Utility functions\n",
    ")\n",
    "\n",
    "# Data and visualization\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: Understanding BGE-M3's Three Modes\n",
    "\n",
    "## Deep Dive: How Each Mode Works\n",
    "\n",
    "### ðŸŽ¯ Dense Retrieval (Single-Vector)\n",
    "\n",
    "**How it works:**\n",
    "- Takes the entire document text\n",
    "- Processes through BERT-based encoder\n",
    "- Extracts [CLS] token embedding\n",
    "- Results in ONE vector (1024 dimensions)\n",
    "\n",
    "**Similarity calculation:**\n",
    "```python\n",
    "similarity = dot_product(query_vector, doc_vector)\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âš¡ Fast: Single vector comparison\n",
    "- ðŸ’¾ Efficient storage: 1024 floats per document\n",
    "- ðŸŽ¯ Good semantic understanding\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Loses fine-grained token information\n",
    "- âŒ May miss exact keyword matches\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Sparse Retrieval (Learned Lexical)\n",
    "\n",
    "**How it works:**\n",
    "- Adds linear layer + ReLU after BERT hidden states\n",
    "- Creates vocabulary-sized vector (e.g., 30,000 dimensions)\n",
    "- Most values are zero (sparse)\n",
    "- Only active terms get non-zero weights\n",
    "\n",
    "**Key difference from BM25:**\n",
    "- BM25: Statistical (TF-IDF, no learning)\n",
    "- BGE-M3 Sparse: Neural (learned from data)\n",
    "\n",
    "**Similarity calculation:**\n",
    "```python\n",
    "similarity = sum(query_weights[i] * doc_weights[i] for i in shared_terms)\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Better than BM25 (learns term importance)\n",
    "- âœ… Handles synonyms and related terms\n",
    "- âœ… Still interpretable (can see which terms match)\n",
    "- ðŸ’¾ Efficient: Only stores non-zero weights\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Slower than BM25 (requires neural inference)\n",
    "- âŒ Needs pre-trained model\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Multi-Vector / ColBERT (Late Interaction)\n",
    "\n",
    "**How it works:**\n",
    "1. Each token in document gets its own vector\n",
    "2. Document with 100 tokens â†’ 100 vectors (1024-dim each)\n",
    "3. At query time: **late interaction**\n",
    "   - For each query token, find max similarity with ALL doc tokens\n",
    "   - Average these max similarities\n",
    "\n",
    "**Similarity calculation (MaxSim):**\n",
    "```python\n",
    "# For each query token\n",
    "for q_token in query_tokens:\n",
    "    # Find max similarity across ALL document tokens\n",
    "    max_sim = max(similarity(q_token, d_token) for d_token in doc_tokens)\n",
    "    scores.append(max_sim)\n",
    "\n",
    "# Average across query tokens\n",
    "final_score = mean(scores)\n",
    "```\n",
    "\n",
    "**Why \"Late\" Interaction?**\n",
    "- Query and document encoded **independently** (early)\n",
    "- Token-level matching happens **at search time** (late)\n",
    "- Allows pre-computing document vectors\n",
    "\n",
    "**Pros:**\n",
    "- â­â­â­ Highest quality retrieval\n",
    "- âœ… Fine-grained token-level matching\n",
    "- âœ… Interpretable (can see which tokens match)\n",
    "- âœ… Better than cross-encoders for retrieval\n",
    "\n",
    "**Cons:**\n",
    "- ðŸŒ Slowest: Must compare many vectors\n",
    "- ðŸ’¾ðŸ’¾ðŸ’¾ Huge storage: 100 vectors per doc (if stored)\n",
    "- âš ï¸ Not practical for initial retrieval at scale\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Feature | Dense | Sparse | Multi-Vector (ColBERT) |\n",
    "|---------|-------|--------|------------------------|\n",
    "| **Vectors per doc** | 1 | 1 (sparse) | N tokens |\n",
    "| **Storage (100-token doc)** | 4 KB | ~1 KB | 400 KB |\n",
    "| **Speed** | âš¡âš¡âš¡ Fast | âš¡âš¡âš¡ Fast | ðŸŒ Slow |\n",
    "| **Quality** | â­â­â­ Good | â­â­â­ Good | â­â­â­â­â­ Excellent |\n",
    "| **Semantic** | âœ… Yes | âœ… Yes | âœ…âœ… Best |\n",
    "| **Keyword** | âŒ Weak | âœ… Good | âœ…âœ… Best |\n",
    "| **Interpretable** | âŒ No | âœ… Yes | âœ…âœ… Yes |\n",
    "| **Best use** | Initial retrieval | Initial retrieval | Reranking |\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Architecture\n",
    "\n",
    "Based on this analysis and IBM research:\n",
    "\n",
    "```\n",
    "Stage 1: Dense + Sparse â†’ Top 50 candidates (FAST, broad recall)\n",
    "Stage 2: ColBERT rerank â†’ Top 5 results (SLOW, high precision)\n",
    "```\n",
    "\n",
    "This gives you:\n",
    "- âš¡ Speed of dense/sparse for initial filtering\n",
    "- â­ Quality of ColBERT for final results\n",
    "- ðŸ’¾ No need to store ColBERT vectors\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: Load Dataset and BGE-M3 Model\n",
    "\n",
    "## Dataset: Climate Fever\n",
    "\n",
    "We'll use the same climate science dataset for consistency with previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading climate_fever dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any data file at /Users/adiyen/temp/backup/advanced-rag-tutorials/climate_fever. Couldn't find 'climate_fever' on the Hugging Face Hub either: LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Load Climate Fever Dataset\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading climate_fever dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclimate_fever\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Sample documents for this demo\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# In production, you'd use more documents\u001b[39;00m\n\u001b[32m     10\u001b[39m documents = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGlobal warming is primarily caused by increased greenhouse gas emissions from human activities, particularly the burning of fossil fuels like coal, oil, and natural gas.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOver the coming 25 or 30 years, scientists say, the climate is likely to gradually warm. However, researchers also say that this phenomenon can be stopped if human emissions are reduced to zero.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mClimate change is disrupting agricultural patterns, forcing farmers to adapt their crops and techniques to new temperature and precipitation regimes that differ from historical norms.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/datasets/load.py:1492\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1487\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1488\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1489\u001b[39m )\n\u001b[32m   1491\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/datasets/load.py:1137\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1147\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/datasets/load.py:1032\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1030\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1031\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1033\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1034\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1035\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Couldn't find any data file at /Users/adiyen/temp/backup/advanced-rag-tutorials/climate_fever. Couldn't find 'climate_fever' on the Hugging Face Hub either: LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "#CELL-NO: 3\n",
    "# ============================================================\n",
    "# Load Climate Fever Dataset\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading climate_fever dataset...\")\n",
    "dataset = load_dataset(\"climate_fever\")\n",
    "\n",
    "# Sample documents for this demo\n",
    "# In production, you'd use more documents\n",
    "documents = [\n",
    "    \"Global warming is primarily caused by increased greenhouse gas emissions from human activities, particularly the burning of fossil fuels like coal, oil, and natural gas.\",\n",
    "    \"Over the coming 25 or 30 years, scientists say, the climate is likely to gradually warm. However, researchers also say that this phenomenon can be stopped if human emissions are reduced to zero.\",\n",
    "    \"The jet stream forms a boundary between the cold north and the warmer south, but the lower temperature difference means the winds are now weaker, leading to more extreme weather patterns.\",\n",
    "    \"Coral reefs become stressed due to ocean acidification and warming, expelling their symbiotic algae which leaves the coral a bleached white color. This process threatens entire marine ecosystems.\",\n",
    "    \"The rapid changes in the climate may have profound consequences for humans and other species. Severe drought caused food shortages for millions of people in Ethiopia, with a lack of rainfall resulting in intense and widespread forest fires.\",\n",
    "    \"Rising sea levels threaten coastal cities worldwide, with predictions suggesting that many major urban centers could face significant flooding by 2100 if current trends continue.\",\n",
    "    \"Melting Arctic ice reduces the Earth's albedo effect, causing the planet to absorb more solar radiation and accelerating the warming process in a dangerous feedback loop.\",\n",
    "    \"Climate change is disrupting agricultural patterns, forcing farmers to adapt their crops and techniques to new temperature and precipitation regimes that differ from historical norms.\"\n",
    "]\n",
    "\n",
    "print(f\"âœ… Loaded {len(documents)} sample documents\")\n",
    "print(\"\\nSample documents:\")\n",
    "for i, doc in enumerate(documents[:3], 1):\n",
    "    print(f\"{i}. {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BGE-M3 Model\n",
    "\n",
    "BGE-M3 is a ~2GB model. First run will download it from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BGE-M3 model...\n",
      "(First run will download ~2GB model from Hugging Face)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c5bb7d4e-e3ac-4886-a431-972e371e8de3)')' thrown while requesting HEAD https://huggingface.co/BAAI/bge-m3/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheck your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/BAAI/bge-m3/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:307\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m response = http_backoff(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:466\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    461\u001b[39m     message = (\n\u001b[32m    462\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 403 Forbidden: None.\nCannot access content at: https://huggingface.co/BAAI/bge-m3/resolve/main/config.json.\nMake sure your token has the correct permissions.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLocalEntryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1114\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[32m   1659\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is on.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1662\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhead_call_error\u001b[39;00m\n",
      "\u001b[31mLocalEntryNotFoundError\u001b[39m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading BGE-M3 model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(First run will download ~2GB model from Hugging Face)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mBGEM3FlagModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBAAI/bge-m3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use half precision for 2x speedup\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Change to 'cuda' if GPU available\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… BGE-M3 model loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModel capabilities:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:89\u001b[39m, in \u001b[36mM3Embedder.__init__\u001b[39m\u001b[34m(self, model_name_or_path, normalize_embeddings, use_fp16, query_instruction_for_retrieval, query_instruction_format, devices, pooling_method, trust_remote_code, cache_dir, colbert_dim, batch_size, query_max_length, passage_max_length, return_dense, return_sparse, return_colbert_vecs, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     73\u001b[39m     model_name_or_path,\n\u001b[32m     74\u001b[39m     normalize_embeddings=normalize_embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     85\u001b[39m     **kwargs\n\u001b[32m     86\u001b[39m )\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.pooling_method = pooling_method\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m.model = EncoderOnlyEmbedderM3ModelForInference(\n\u001b[32m     95\u001b[39m     EncoderOnlyEmbedderM3Runner.get_model(\n\u001b[32m     96\u001b[39m         model_name_or_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m     normalize_embeddings=normalize_embeddings\n\u001b[32m    104\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1109\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1107\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1332\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1330\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1334\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:662\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:721\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    717\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/temp/backup/advanced-rag-tutorials/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:553\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;66;03m# Here we only raise if both flags for missing entry and connection errors are True (because it can be raised\u001b[39;00m\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    554\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWe couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt connect to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m to load the files, and couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find them in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m cached files.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCheck your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# later on anyway and re-raised if needed\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, HTTPError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n",
      "\u001b[31mOSError\u001b[39m: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheck your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "#CELL-NO: 4\n",
    "# ============================================================\n",
    "# Load BGE-M3 Model\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading BGE-M3 model...\")\n",
    "print(\"(First run will download ~2GB model from Hugging Face)\\n\")\n",
    "\n",
    "model = BGEM3FlagModel(\n",
    "    'BAAI/bge-m3',\n",
    "    use_fp16=True,  # Use half precision for 2x speedup\n",
    "    device='cpu'     # Change to 'cuda' if GPU available\n",
    ")\n",
    "\n",
    "print(\"âœ… BGE-M3 model loaded successfully!\")\n",
    "print(\"\\nModel capabilities:\")\n",
    "print(\"  - Dense embeddings: 1024 dimensions\")\n",
    "print(\"  - Sparse embeddings: ~30,000 dimensions (mostly zeros)\")\n",
    "print(\"  - Multi-vector (ColBERT): N tokens Ã— 1024 dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4: Generate All Three Embedding Types\n",
    "\n",
    "## BGE-M3 Multi-Functional Encoding\n",
    "\n",
    "The powerful feature of BGE-M3 is that it can generate **all three embedding types in a single forward pass**!\n",
    "\n",
    "### What happens under the hood:\n",
    "1. **Input text** â†’ BERT encoder â†’ **hidden states**\n",
    "2. **Dense**: Extract [CLS] token â†’ 1024-dim vector\n",
    "3. **Sparse**: Linear layer + ReLU on hidden states â†’ vocabulary-sized sparse vector\n",
    "4. **ColBERT**: All token hidden states â†’ N Ã— 1024-dim multi-vector\n",
    "\n",
    "Let's generate all three types for our documents and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 5\n",
    "# ============================================================\n",
    "# Generate All Three Embedding Types for Documents\n",
    "# ============================================================\n",
    "\n",
    "print(\"Generating embeddings for documents...\")\n",
    "print(\"This will generate: Dense + Sparse + ColBERT vectors\\n\")\n",
    "\n",
    "# BGE-M3 encode() can return all three types at once!\n",
    "doc_embeddings = model.encode(\n",
    "    documents,\n",
    "    batch_size=12,\n",
    "    max_length=512,             # Maximum sequence length\n",
    "    return_dense=True,          # âœ… Dense embeddings (1024-dim)\n",
    "    return_sparse=True,         # âœ… Sparse embeddings (vocab-size, mostly zeros)\n",
    "    return_colbert_vecs=False   # âŒ Don't store ColBERT (compute on-the-fly later)\n",
    ")\n",
    "\n",
    "# Extract each embedding type\n",
    "dense_vecs = doc_embeddings['dense_vecs']         # numpy array: (8, 1024)\n",
    "sparse_vecs = doc_embeddings['lexical_weights']   # list of dicts: [{token_id: weight}, ...]\n",
    "\n",
    "print(\"âœ… Document embeddings generated!\")\n",
    "print(f\"\\nDense vectors shape: {dense_vecs.shape}\")\n",
    "print(f\"Sparse vectors: {len(sparse_vecs)} documents\")\n",
    "print(f\"\\nSample sparse vector (first 10 terms):\")\n",
    "sample_sparse = list(sparse_vecs[0].items())[:10]\n",
    "for token_id, weight in sample_sparse:\n",
    "    print(f\"  Token {token_id}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5: Milvus Multi-Vector Schema Setup\n",
    "\n",
    "## Storage Strategy\n",
    "\n",
    "We'll store **dense** and **sparse** vectors in Milvus for fast candidate retrieval.\n",
    "\n",
    "**Why not store ColBERT vectors?**\n",
    "- A 100-token document needs 100 Ã— 1024-dim vectors = **400 KB** per document\n",
    "- For 1M documents: **400 GB** just for ColBERT vectors!\n",
    "- Instead: Compute ColBERT on-the-fly only for reranking (top-50 candidates)\n",
    "\n",
    "## Schema Design\n",
    "\n",
    "Our collection will have:\n",
    "1. **Primary key** (auto-generated ID)\n",
    "2. **Text field** (original document text)\n",
    "3. **Dense vector** (1024 dimensions, FLOAT_VECTOR)\n",
    "4. **Sparse vector** (SPARSE_FLOAT_VECTOR)\n",
    "\n",
    "## Index Types\n",
    "- **Dense**: IVF_FLAT (fast approximate search)\n",
    "- **Sparse**: SPARSE_INVERTED_INDEX (efficient sparse retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 6\n",
    "# ============================================================\n",
    "# Connect to Milvus and Create Collection\n",
    "# ============================================================\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\n",
    "    alias=\"default\",\n",
    "    uri=\"http://localhost:19530\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Connected to Milvus\")\n",
    "\n",
    "# Collection name\n",
    "collection_name = \"bge_m3_hybrid_demo\"\n",
    "\n",
    "# Drop existing collection if it exists\n",
    "if utility.has_collection(collection_name):\n",
    "    utility.drop_collection(collection_name)\n",
    "    print(f\"ðŸ—‘ï¸  Dropped existing collection: {collection_name}\")\n",
    "\n",
    "# Define schema fields\n",
    "fields = [\n",
    "    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=2000),\n",
    "    FieldSchema(name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=1024),\n",
    "    FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "]\n",
    "\n",
    "# Create schema\n",
    "schema = CollectionSchema(\n",
    "    fields=fields,\n",
    "    description=\"BGE-M3 multi-vector hybrid retrieval demo\"\n",
    ")\n",
    "\n",
    "# Create collection\n",
    "collection = Collection(\n",
    "    name=collection_name,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 7\n",
    "# ============================================================\n",
    "# Create Indexes for Dense and Sparse Vectors\n",
    "# ============================================================\n",
    "\n",
    "print(\"Creating indexes...\")\n",
    "\n",
    "# Dense vector index - IVF_FLAT for fast approximate nearest neighbor search\n",
    "dense_index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"IP\",       # Inner Product (cosine similarity for normalized vectors)\n",
    "    \"params\": {\"nlist\": 128}   # Number of clusters\n",
    "}\n",
    "\n",
    "collection.create_index(\n",
    "    field_name=\"dense_vector\",\n",
    "    index_params=dense_index_params\n",
    ")\n",
    "print(\"âœ… Created dense vector index (IVF_FLAT)\")\n",
    "\n",
    "# Sparse vector index - Inverted index for sparse retrieval\n",
    "sparse_index_params = {\n",
    "    \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "    \"metric_type\": \"IP\"        # Inner Product for sparse vectors\n",
    "}\n",
    "\n",
    "collection.create_index(\n",
    "    field_name=\"sparse_vector\",\n",
    "    index_params=sparse_index_params\n",
    ")\n",
    "print(\"âœ… Created sparse vector index (SPARSE_INVERTED_INDEX)\")\n",
    "\n",
    "# Load collection into memory\n",
    "collection.load()\n",
    "print(\"âœ… Collection loaded into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 8\n",
    "# ============================================================\n",
    "# Insert Documents with Dense and Sparse Embeddings\n",
    "# ============================================================\n",
    "\n",
    "print(\"Inserting documents into Milvus...\")\n",
    "\n",
    "# Prepare data for insertion\n",
    "# Note: BGE-M3 sparse vectors are already in dict format (Milvus-compatible)\n",
    "entities = [\n",
    "    documents,           # text field\n",
    "    dense_vecs.tolist(), # dense_vector field (convert numpy to list)\n",
    "    sparse_vecs          # sparse_vector field (already dict format)\n",
    "]\n",
    "\n",
    "# Insert data\n",
    "insert_result = collection.insert(entities)\n",
    "\n",
    "print(f\"âœ… Inserted {len(documents)} documents into Milvus\")\n",
    "print(f\"   Primary keys: {insert_result.primary_keys[:3]}... (showing first 3)\")\n",
    "\n",
    "# Flush to ensure data is written\n",
    "collection.flush()\n",
    "print(\"âœ… Data flushed to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 6: Implement Retrieval Functions\n",
    "\n",
    "Now we'll implement five different retrieval strategies:\n",
    "\n",
    "1. **Dense-Only Search**: Uses only dense embeddings (semantic search)\n",
    "2. **Sparse-Only Search**: Uses only sparse embeddings (learned lexical search)\n",
    "3. **Dense + Sparse Hybrid**: Combines both with RRF (best balance)\n",
    "4. **ColBERT Reranking**: Token-level late interaction for maximum quality\n",
    "5. **Full Pipeline**: Dense+Sparse candidates â†’ ColBERT reranking\n",
    "\n",
    "Let's implement each one step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 9\n",
    "# ============================================================\n",
    "# Function 1: Dense-Only Search\n",
    "# ============================================================\n",
    "\n",
    "def search_dense(query: str, limit: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search using only dense embeddings (semantic search).\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        limit: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with scores and text\n",
    "    \"\"\"\n",
    "    # Generate dense embedding for query\n",
    "    query_embeddings = model.encode(\n",
    "        [query],\n",
    "        return_dense=True,\n",
    "        return_sparse=False,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    query_dense = query_embeddings['dense_vecs'][0]\n",
    "    \n",
    "    # Search using only dense vector\n",
    "    results = collection.search(\n",
    "        data=[query_dense.tolist()],\n",
    "        anns_field=\"dense_vector\",\n",
    "        param={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}},\n",
    "        limit=limit,\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for hits in results:\n",
    "        for hit in hits:\n",
    "            formatted_results.append({\n",
    "                \"text\": hit.entity.get(\"text\"),\n",
    "                \"score\": hit.score,\n",
    "                \"rank\": len(formatted_results) + 1\n",
    "            })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "print(\"âœ… Defined search_dense() function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 10\n",
    "# ============================================================\n",
    "# Function 2: Sparse-Only Search\n",
    "# ============================================================\n",
    "\n",
    "def search_sparse(query: str, limit: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search using only sparse embeddings (learned lexical search).\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        limit: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with scores and text\n",
    "    \"\"\"\n",
    "    # Generate sparse embedding for query\n",
    "    query_embeddings = model.encode(\n",
    "        [query],\n",
    "        return_dense=False,\n",
    "        return_sparse=True,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    query_sparse = query_embeddings['lexical_weights'][0]\n",
    "    \n",
    "    # Search using only sparse vector\n",
    "    results = collection.search(\n",
    "        data=[query_sparse],\n",
    "        anns_field=\"sparse_vector\",\n",
    "        param={\"metric_type\": \"IP\"},\n",
    "        limit=limit,\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for hits in results:\n",
    "        for hit in hits:\n",
    "            formatted_results.append({\n",
    "                \"text\": hit.entity.get(\"text\"),\n",
    "                \"score\": hit.score,\n",
    "                \"rank\": len(formatted_results) + 1\n",
    "            })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "print(\"âœ… Defined search_sparse() function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 11\n",
    "# ============================================================\n",
    "# Function 3: Dense + Sparse Hybrid Search with RRF\n",
    "# ============================================================\n",
    "\n",
    "def search_hybrid(query: str, limit: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Hybrid search combining dense and sparse embeddings with RRF.\n",
    "    \n",
    "    This is the recommended approach for production RAG systems.\n",
    "    Reciprocal Rank Fusion (RRF) combines rankings from both methods.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        limit: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with scores and text\n",
    "    \"\"\"\n",
    "    # Generate both dense and sparse embeddings for query\n",
    "    query_embeddings = model.encode(\n",
    "        [query],\n",
    "        return_dense=True,\n",
    "        return_sparse=True,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    query_dense = query_embeddings['dense_vecs'][0]\n",
    "    query_sparse = query_embeddings['lexical_weights'][0]\n",
    "    \n",
    "    # Create search requests for both dense and sparse\n",
    "    dense_search_params = {\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}\n",
    "    sparse_search_params = {\"metric_type\": \"IP\"}\n",
    "    \n",
    "    # Dense search request\n",
    "    dense_req = AnnSearchRequest(\n",
    "        data=[query_dense.tolist()],\n",
    "        anns_field=\"dense_vector\",\n",
    "        param=dense_search_params,\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    # Sparse search request\n",
    "    sparse_req = AnnSearchRequest(\n",
    "        data=[query_sparse],\n",
    "        anns_field=\"sparse_vector\",\n",
    "        param=sparse_search_params,\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    # Hybrid search with RRF (Reciprocal Rank Fusion)\n",
    "    # RRF formula: score = sum(1 / (k + rank_i)) for each method\n",
    "    # k=60 is a common default that works well\n",
    "    results = collection.hybrid_search(\n",
    "        reqs=[dense_req, sparse_req],\n",
    "        rerank=RRFRanker(k=60),\n",
    "        limit=limit,\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for hits in results:\n",
    "        for hit in hits:\n",
    "            formatted_results.append({\n",
    "                \"text\": hit.entity.get(\"text\"),\n",
    "                \"score\": hit.score,\n",
    "                \"rank\": len(formatted_results) + 1\n",
    "            })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "print(\"âœ… Defined search_hybrid() function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 12\n",
    "# ============================================================\n",
    "# Function 4: ColBERT Reranking (Late Interaction)\n",
    "# ============================================================\n",
    "\n",
    "def rerank_with_colbert(query: str, candidate_texts: List[str], top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Rerank candidate documents using ColBERT multi-vector late interaction.\n",
    "    \n",
    "    This is EXPENSIVE - only use on a small set of candidates (e.g., top-20 from hybrid search).\n",
    "    \n",
    "    How it works:\n",
    "    1. Query â†’ N query token vectors (e.g., 10 tokens â†’ 10 Ã— 1024-dim vectors)\n",
    "    2. Each document â†’ M doc token vectors (e.g., 100 tokens â†’ 100 Ã— 1024-dim vectors)\n",
    "    3. For each query token:\n",
    "       - Compute similarity with ALL document tokens\n",
    "       - Take the MAX similarity (best matching doc token)\n",
    "    4. Average MAX similarities across all query tokens â†’ final score\n",
    "    \n",
    "    This is called \"MaxSim\" late interaction.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        candidate_texts: List of candidate document texts\n",
    "        top_k: Number of top results to return after reranking\n",
    "    \n",
    "    Returns:\n",
    "        List of reranked results with ColBERT scores\n",
    "    \"\"\"\n",
    "    # Generate ColBERT vectors for query\n",
    "    query_embeddings = model.encode(\n",
    "        [query],\n",
    "        return_dense=False,\n",
    "        return_sparse=False,\n",
    "        return_colbert_vecs=True\n",
    "    )\n",
    "    query_vecs = query_embeddings['colbert_vecs'][0]  # Shape: (n_query_tokens, 1024)\n",
    "    \n",
    "    # Generate ColBERT vectors for all candidate documents\n",
    "    doc_embeddings = model.encode(\n",
    "        candidate_texts,\n",
    "        return_dense=False,\n",
    "        return_sparse=False,\n",
    "        return_colbert_vecs=True\n",
    "    )\n",
    "    doc_vecs_list = doc_embeddings['colbert_vecs']  # List of arrays\n",
    "    \n",
    "    # Compute ColBERT scores using MaxSim late interaction\n",
    "    scores = []\n",
    "    for doc_vecs in doc_vecs_list:\n",
    "        # doc_vecs shape: (n_doc_tokens, 1024)\n",
    "        # query_vecs shape: (n_query_tokens, 1024)\n",
    "        \n",
    "        # Compute similarity matrix: (n_query_tokens, n_doc_tokens)\n",
    "        sim_matrix = np.dot(query_vecs, doc_vecs.T)\n",
    "        \n",
    "        # MaxSim: For each query token, find max similarity across all doc tokens\n",
    "        max_sims = np.max(sim_matrix, axis=1)  # Shape: (n_query_tokens,)\n",
    "        \n",
    "        # Average max similarities across query tokens\n",
    "        score = np.mean(max_sims)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Sort documents by ColBERT score (descending)\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    # Format results\n",
    "    reranked_results = []\n",
    "    for rank, idx in enumerate(ranked_indices, 1):\n",
    "        reranked_results.append({\n",
    "            \"text\": candidate_texts[idx],\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"rank\": rank\n",
    "        })\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "print(\"âœ… Defined rerank_with_colbert() function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 13\n",
    "# ============================================================\n",
    "# Function 5: Full Pipeline (Hybrid â†’ ColBERT Reranking)\n",
    "# ============================================================\n",
    "\n",
    "def search_full_pipeline(query: str, candidate_limit: int = 20, final_limit: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Complete two-stage retrieval pipeline (RECOMMENDED for production).\n",
    "    \n",
    "    Stage 1: Dense + Sparse hybrid search â†’ Get top-N candidates (fast, broad recall)\n",
    "    Stage 2: ColBERT reranking â†’ Refine to top-K results (slow, high precision)\n",
    "    \n",
    "    This architecture balances quality and performance:\n",
    "    - Hybrid search quickly narrows down from thousands/millions of docs to ~20-50\n",
    "    - ColBERT only needs to process these few candidates (cheap!)\n",
    "    - Final results have state-of-the-art quality\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        candidate_limit: Number of candidates from hybrid search (default: 20)\n",
    "        final_limit: Final number of results after reranking (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        List of final reranked results with ColBERT scores\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Stage 1: Hybrid search (dense + sparse) â†’ retrieving top-{candidate_limit} candidates...\")\n",
    "    \n",
    "    # Stage 1: Get candidates with hybrid search\n",
    "    candidates = search_hybrid(query, limit=candidate_limit)\n",
    "    candidate_texts = [c[\"text\"] for c in candidates]\n",
    "    \n",
    "    print(f\"   âœ… Retrieved {len(candidates)} candidates\")\n",
    "    print(f\"\\nðŸ§  Stage 2: ColBERT reranking â†’ refining to top-{final_limit} results...\")\n",
    "    \n",
    "    # Stage 2: Rerank with ColBERT\n",
    "    final_results = rerank_with_colbert(query, candidate_texts, top_k=final_limit)\n",
    "    \n",
    "    print(f\"   âœ… Reranked to {len(final_results)} final results\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "print(\"âœ… Defined search_full_pipeline() function\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All 5 retrieval functions are ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 7: Query Demonstrations\n",
    "\n",
    "Now let's test all our retrieval methods with different types of queries:\n",
    "\n",
    "## Query Types\n",
    "\n",
    "1. **Semantic Query**: Tests understanding of meaning (dense should excel)\n",
    "2. **Keyword Query**: Tests lexical matching (sparse should excel)\n",
    "3. **Complex Query**: Tests overall quality (full pipeline should excel)\n",
    "\n",
    "For each query, we'll compare:\n",
    "- Dense-only search\n",
    "- Sparse-only search\n",
    "- Hybrid (Dense + Sparse with RRF)\n",
    "- Full pipeline (Hybrid â†’ ColBERT reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 14\n",
    "# ============================================================\n",
    "# Helper Function to Display Results\n",
    "# ============================================================\n",
    "\n",
    "def display_results(results: List[Dict], method_name: str):\n",
    "    \"\"\"Pretty print search results.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“Š {method_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\nRank {result['rank']} | Score: {result['score']:.4f}\")\n",
    "        text = result['text']\n",
    "        # Truncate long text\n",
    "        if len(text) > 150:\n",
    "            text = text[:150] + \"...\"\n",
    "        print(f\"   {text}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"âœ… Helper function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Query 1: Semantic Query\n",
    "\n",
    "**Query**: \"How does global warming affect our planet?\"\n",
    "\n",
    "This query uses natural language and tests semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 15\n",
    "# ============================================================\n",
    "# Query 1: Semantic Query\n",
    "# ============================================================\n",
    "\n",
    "query1 = \"How does global warming affect our planet?\"\n",
    "\n",
    "print(f\"ðŸ” Query: '{query1}'\\n\")\n",
    "\n",
    "# Test all methods\n",
    "results_dense_q1 = search_dense(query1, limit=3)\n",
    "results_sparse_q1 = search_sparse(query1, limit=3)\n",
    "results_hybrid_q1 = search_hybrid(query1, limit=3)\n",
    "\n",
    "# Display results\n",
    "display_results(results_dense_q1, \"Method 1: Dense-Only Search\")\n",
    "display_results(results_sparse_q1, \"Method 2: Sparse-Only Search\")\n",
    "display_results(results_hybrid_q1, \"Method 3: Hybrid (Dense + Sparse + RRF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 16\n",
    "# ============================================================\n",
    "# Query 1: Full Pipeline with ColBERT Reranking\n",
    "# ============================================================\n",
    "\n",
    "print(f\"ðŸš€ Running FULL PIPELINE for Query 1...\\n\")\n",
    "\n",
    "results_full_q1 = search_full_pipeline(query1, candidate_limit=8, final_limit=3)\n",
    "\n",
    "display_results(results_full_q1, \"Method 4: Full Pipeline (Hybrid â†’ ColBERT Reranking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Query 2: Keyword Query\n",
    "\n",
    "**Query**: \"fossil fuels emissions greenhouse gas\"\n",
    "\n",
    "This query uses specific keywords and tests lexical matching (sparse should excel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 17\n",
    "# ============================================================\n",
    "# Query 2: Keyword Query\n",
    "# ============================================================\n",
    "\n",
    "query2 = \"fossil fuels emissions greenhouse gas\"\n",
    "\n",
    "print(f\"ðŸ” Query: '{query2}'\\n\")\n",
    "\n",
    "# Test all methods\n",
    "results_dense_q2 = search_dense(query2, limit=3)\n",
    "results_sparse_q2 = search_sparse(query2, limit=3)\n",
    "results_hybrid_q2 = search_hybrid(query2, limit=3)\n",
    "\n",
    "# Display results\n",
    "display_results(results_dense_q2, \"Method 1: Dense-Only Search\")\n",
    "display_results(results_sparse_q2, \"Method 2: Sparse-Only Search\")\n",
    "display_results(results_hybrid_q2, \"Method 3: Hybrid (Dense + Sparse + RRF)\")\n",
    "\n",
    "# Full pipeline\n",
    "print(f\"\\nðŸš€ Running FULL PIPELINE for Query 2...\\n\")\n",
    "results_full_q2 = search_full_pipeline(query2, candidate_limit=8, final_limit=3)\n",
    "display_results(results_full_q2, \"Method 4: Full Pipeline (Hybrid â†’ ColBERT Reranking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Query 3: Complex Query\n",
    "\n",
    "**Query**: \"What are the feedback loops and cascading effects of climate change on ecosystems?\"\n",
    "\n",
    "This complex query tests overall retrieval quality (full pipeline should excel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 18\n",
    "# ============================================================\n",
    "# Query 3: Complex Query\n",
    "# ============================================================\n",
    "\n",
    "query3 = \"What are the feedback loops and cascading effects of climate change on ecosystems?\"\n",
    "\n",
    "print(f\"ðŸ” Query: '{query3}'\\n\")\n",
    "\n",
    "# Test all methods\n",
    "results_dense_q3 = search_dense(query3, limit=3)\n",
    "results_sparse_q3 = search_sparse(query3, limit=3)\n",
    "results_hybrid_q3 = search_hybrid(query3, limit=3)\n",
    "\n",
    "# Display results\n",
    "display_results(results_dense_q3, \"Method 1: Dense-Only Search\")\n",
    "display_results(results_sparse_q3, \"Method 2: Sparse-Only Search\")\n",
    "display_results(results_hybrid_q3, \"Method 3: Hybrid (Dense + Sparse + RRF)\")\n",
    "\n",
    "# Full pipeline\n",
    "print(f\"\\nðŸš€ Running FULL PIPELINE for Query 3...\\n\")\n",
    "results_full_q3 = search_full_pipeline(query3, candidate_limit=8, final_limit=3)\n",
    "display_results(results_full_q3, \"Method 4: Full Pipeline (Hybrid â†’ ColBERT Reranking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 8: Visualizations and Analysis\n",
    "\n",
    "Let's create visualizations to better understand the differences between retrieval methods:\n",
    "\n",
    "1. **Score Comparison**: Compare scores across all methods for each query\n",
    "2. **ColBERT Token Attribution**: Visualize which query tokens match which document tokens\n",
    "3. **Method Performance Summary**: Overall comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 19\n",
    "# ============================================================\n",
    "# Visualization 1: Score Comparison Across Methods\n",
    "# ============================================================\n",
    "\n",
    "def visualize_score_comparison(dense_res, sparse_res, hybrid_res, colbert_res, query_name):\n",
    "    \"\"\"\n",
    "    Compare scores across all four methods.\n",
    "    \n",
    "    Note: Scores are NOT directly comparable (different ranges).\n",
    "    We'll normalize them to [0, 1] for visual comparison.\n",
    "    \"\"\"\n",
    "    # Normalize scores to [0, 1] for each method\n",
    "    def normalize(scores):\n",
    "        if len(scores) == 0:\n",
    "            return []\n",
    "        min_s, max_s = min(scores), max(scores)\n",
    "        if max_s == min_s:\n",
    "            return [1.0] * len(scores)\n",
    "        return [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "    \n",
    "    # Extract scores (top 3 only)\n",
    "    dense_scores = normalize([r['score'] for r in dense_res[:3]])\n",
    "    sparse_scores = normalize([r['score'] for r in sparse_res[:3]])\n",
    "    hybrid_scores = normalize([r['score'] for r in hybrid_res[:3]])\n",
    "    colbert_scores = normalize([r['score'] for r in colbert_res[:3]])\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(3)  # 3 results\n",
    "    width = 0.2\n",
    "    \n",
    "    bars1 = ax.bar(x - 1.5*width, dense_scores, width, label='Dense', alpha=0.8, color='#3498db')\n",
    "    bars2 = ax.bar(x - 0.5*width, sparse_scores, width, label='Sparse', alpha=0.8, color='#e74c3c')\n",
    "    bars3 = ax.bar(x + 0.5*width, hybrid_scores, width, label='Hybrid', alpha=0.8, color='#2ecc71')\n",
    "    bars4 = ax.bar(x + 1.5*width, colbert_scores, width, label='ColBERT', alpha=0.8, color='#f39c12')\n",
    "    \n",
    "    ax.set_xlabel('Rank', fontsize=12)\n",
    "    ax.set_ylabel('Normalized Score (0-1)', fontsize=12)\n",
    "    ax.set_title(f'Score Comparison: {query_name}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Rank 1', 'Rank 2', 'Rank 3'])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize for Query 1\n",
    "visualize_score_comparison(\n",
    "    results_dense_q1, \n",
    "    results_sparse_q1, \n",
    "    results_hybrid_q1, \n",
    "    results_full_q1,\n",
    "    \"Query 1 (Semantic)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 20\n",
    "# ============================================================\n",
    "# Visualization 2: ColBERT Token Attribution Heatmap\n",
    "# ============================================================\n",
    "\n",
    "def visualize_colbert_attribution(query: str, document: str, max_tokens: int = 20):\n",
    "    \"\"\"\n",
    "    Visualize which query tokens match which document tokens in ColBERT.\n",
    "    \n",
    "    This creates a heatmap showing token-level similarities.\n",
    "    Darker colors = higher similarity between query and document tokens.\n",
    "    \"\"\"\n",
    "    # Generate ColBERT vectors\n",
    "    query_emb = model.encode([query], return_colbert_vecs=True)\n",
    "    doc_emb = model.encode([document], return_colbert_vecs=True)\n",
    "    \n",
    "    query_vecs = query_emb['colbert_vecs'][0]  # (n_query_tokens, 1024)\n",
    "    doc_vecs = doc_emb['colbert_vecs'][0]      # (n_doc_tokens, 1024)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = np.dot(query_vecs, doc_vecs.T)  # (n_query_tokens, n_doc_tokens)\n",
    "    \n",
    "    # Limit to max_tokens for readability\n",
    "    if sim_matrix.shape[0] > max_tokens:\n",
    "        sim_matrix = sim_matrix[:max_tokens, :]\n",
    "    if sim_matrix.shape[1] > max_tokens:\n",
    "        sim_matrix = sim_matrix[:, :max_tokens]\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    im = ax.imshow(sim_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    \n",
    "    ax.set_xlabel('Document Tokens', fontsize=12)\n",
    "    ax.set_ylabel('Query Tokens', fontsize=12)\n",
    "    ax.set_title('ColBERT Token-Level Similarity Matrix', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Similarity Score', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add grid\n",
    "    ax.set_xticks(np.arange(sim_matrix.shape[1]))\n",
    "    ax.set_yticks(np.arange(sim_matrix.shape[0]))\n",
    "    ax.grid(which='both', color='white', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show MaxSim explanation\n",
    "    max_sims = np.max(sim_matrix, axis=1)\n",
    "    colbert_score = np.mean(max_sims)\n",
    "    print(f\"\\\\nColBERT Score (mean of max similarities): {colbert_score:.4f}\")\n",
    "    print(f\"Max similarities per query token: {max_sims[:5]}... (showing first 5)\")\n",
    "\n",
    "# Example: Visualize top result from full pipeline\n",
    "print(\"Visualizing ColBERT token attribution for Query 1, Top Result:\\\\n\")\n",
    "top_doc = results_full_q1[0]['text']\n",
    "visualize_colbert_attribution(query1, top_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 9: Understanding Trade-offs\n",
    "\n",
    "Now that we've seen all four retrieval methods in action, let's analyze their trade-offs.\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Method | Quality | Speed | Storage | Best Use Case |\n",
    "|--------|---------|-------|---------|---------------|\n",
    "| **Dense-only** | â­â­â­ Good | âš¡âš¡âš¡ Very Fast | ðŸ’¾ Low (4 KB/doc) | Quick semantic search |\n",
    "| **Sparse-only** | â­â­â­ Good | âš¡âš¡âš¡ Very Fast | ðŸ’¾ Low (~1 KB/doc) | Keyword + semantic |\n",
    "| **Hybrid (Dense + Sparse)** | â­â­â­â­ Very Good | âš¡âš¡ Fast | ðŸ’¾ Low (~5 KB/doc) | **Production RAG (Stage 1)** |\n",
    "| **Full Pipeline (+ ColBERT)** | â­â­â­â­â­ Excellent | âš¡ Moderate | ðŸ’¾ Low* | **Best Quality (Stage 2)** |\n",
    "\n",
    "*ColBERT vectors computed on-the-fly, not stored.\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### 1. Dense vs Sparse\n",
    "\n",
    "**Dense embeddings** excel at:\n",
    "- Semantic similarity (synonyms, paraphrases)\n",
    "- Abstract concepts\n",
    "- Cross-lingual retrieval\n",
    "\n",
    "**Sparse embeddings** excel at:\n",
    "- Exact keyword matching\n",
    "- Domain-specific terminology\n",
    "- Named entities\n",
    "\n",
    "**Hybrid (RRF)** gets the best of both worlds!\n",
    "\n",
    "### 2. Why Not Store ColBERT Vectors?\n",
    "\n",
    "Storage comparison for 100-token document:\n",
    "- Dense: 1024 floats Ã— 4 bytes = **4 KB**\n",
    "- Sparse: ~50 non-zero weights Ã— 8 bytes = **~400 bytes**\n",
    "- ColBERT: 100 tokens Ã— 1024 floats Ã— 4 bytes = **400 KB**\n",
    "\n",
    "For 1 million documents:\n",
    "- Dense + Sparse: **~5 GB**\n",
    "- ColBERT: **400 GB** (80x larger!)\n",
    "\n",
    "**Solution**: Compute ColBERT on-the-fly only for reranking (20-50 candidates).\n",
    "\n",
    "### 3. Optimal Two-Stage Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 1: Candidate Retrieval           â”‚\n",
    "â”‚  Method: Dense + Sparse Hybrid (RRF)    â”‚\n",
    "â”‚  Speed: Very Fast (50-100ms)            â”‚\n",
    "â”‚  Output: Top-50 candidates               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 2: Precision Reranking           â”‚\n",
    "â”‚  Method: ColBERT Late Interaction       â”‚\n",
    "â”‚  Speed: Moderate (100-200ms)            â”‚\n",
    "â”‚  Output: Top-5 final results            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Stage 1 quickly filters millions â†’ ~50 (broad recall)\n",
    "- Stage 2 only processes 50 docs (high precision)\n",
    "- Total latency: ~200ms (acceptable for most applications)\n",
    "- Quality: State-of-the-art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 10: Production Architecture Recommendations\n",
    "\n",
    "Based on IBM's \"Blended RAG\" research and our experiments, here are production-ready architectures for different scenarios.\n",
    "\n",
    "## Scenario 1: Maximum Quality (Recommended)\n",
    "\n",
    "**Use case**: High-stakes applications where quality matters most (legal, medical, research)\n",
    "\n",
    "```\n",
    "Query â†’ BGE-M3 Encode (Dense + Sparse + ColBERT)\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 1: Dense + Sparse Hybrid (RRF)        â”‚\n",
    "â”‚  - Milvus hybrid_search()                    â”‚\n",
    "â”‚  - Retrieve top-50 candidates                â”‚\n",
    "â”‚  - Latency: ~50ms                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 2: ColBERT Reranking                  â”‚\n",
    "â”‚  - Compute late interaction on-the-fly       â”‚\n",
    "â”‚  - Rerank top-50 â†’ top-5                     â”‚\n",
    "â”‚  - Latency: ~150ms                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "Top-5 results (total latency: ~200ms)\n",
    "```\n",
    "\n",
    "**Expected performance:**\n",
    "- NDCG@10: ~0.87 (IBM research)\n",
    "- Latency: 200-300ms\n",
    "- Storage: ~5 KB per document\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario 2: Balanced (Fast & Good)\n",
    "\n",
    "**Use case**: Most production RAG systems\n",
    "\n",
    "```\n",
    "Query â†’ BGE-M3 Encode (Dense + Sparse only)\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Dense + Sparse Hybrid (RRF)                 â”‚\n",
    "â”‚  - Milvus hybrid_search()                    â”‚\n",
    "â”‚  - Retrieve top-5 directly                   â”‚\n",
    "â”‚  - Latency: ~50ms                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "Top-5 results (total latency: ~50ms)\n",
    "```\n",
    "\n",
    "**Expected performance:**\n",
    "- NDCG@10: ~0.82 (good quality)\n",
    "- Latency: 50-100ms (4x faster)\n",
    "- Storage: ~5 KB per document\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario 3: Maximum Speed (Simple)\n",
    "\n",
    "**Use case**: Real-time applications, low-latency requirements\n",
    "\n",
    "```\n",
    "Query â†’ BGE-M3 Encode (Dense only)\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Dense Vector Search                         â”‚\n",
    "â”‚  - Milvus search()                           â”‚\n",
    "â”‚  - Retrieve top-5 directly                   â”‚\n",
    "â”‚  - Latency: ~20ms                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "Top-5 results (total latency: ~20ms)\n",
    "```\n",
    "\n",
    "**Expected performance:**\n",
    "- NDCG@10: ~0.75 (acceptable)\n",
    "- Latency: 20-30ms (10x faster)\n",
    "- Storage: ~4 KB per document\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Checklist\n",
    "\n",
    "For production deployment:\n",
    "\n",
    "### âœ… Model Serving\n",
    "- [ ] Load BGE-M3 model once at startup (not per request)\n",
    "- [ ] Use GPU for faster encoding (5-10x speedup)\n",
    "- [ ] Consider model quantization (FP16 or INT8) for lower memory\n",
    "- [ ] Batch queries when possible (higher throughput)\n",
    "\n",
    "### âœ… Milvus Configuration\n",
    "- [ ] Use SSD for faster disk I/O\n",
    "- [ ] Tune `nlist` and `nprobe` for speed/quality trade-off\n",
    "- [ ] Enable query result cache for repeated queries\n",
    "- [ ] Set up replicas for high availability\n",
    "\n",
    "### âœ… Monitoring\n",
    "- [ ] Track latency percentiles (p50, p95, p99)\n",
    "- [ ] Monitor retrieval quality metrics (NDCG, MRR)\n",
    "- [ ] Log failed queries for analysis\n",
    "- [ ] Set up alerts for degraded performance\n",
    "\n",
    "### âœ… Optimization\n",
    "- [ ] Cache ColBERT results for popular queries\n",
    "- [ ] Use async processing for non-blocking retrieval\n",
    "- [ ] Implement request batching for efficiency\n",
    "- [ ] Consider approximate ColBERT for speed (e.g., pruning)\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Each Architecture\n",
    "\n",
    "| Requirement | Recommended Architecture |\n",
    "|-------------|--------------------------|\n",
    "| **Quality is critical** | Scenario 1 (Hybrid + ColBERT) |\n",
    "| **Balanced quality & speed** | Scenario 2 (Hybrid only) |\n",
    "| **Real-time, low latency** | Scenario 3 (Dense only) |\n",
    "| **Keyword-heavy queries** | Scenario 2 or 1 (needs sparse) |\n",
    "| **Semantic queries** | Scenario 2 or 3 (dense/hybrid) |\n",
    "| **Large scale (10M+ docs)** | Scenario 2 (hybrid without ColBERT) |\n",
    "| **Small scale (<100K docs)** | Scenario 1 (can afford ColBERT) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 11: Key Takeaways\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "Congratulations! You've mastered state-of-the-art multi-vector retrieval with BGE-M3. Here's what you now understand:\n",
    "\n",
    "### ðŸŽ¯ Core Concepts\n",
    "\n",
    "1. **BGE-M3's Three Modes**:\n",
    "   - Dense retrieval (semantic, 1024-dim)\n",
    "   - Sparse retrieval (learned lexical, vocabulary-sized)\n",
    "   - ColBERT multi-vector (token-level late interaction)\n",
    "\n",
    "2. **Why Multi-Way Retrieval Works**:\n",
    "   - Dense captures semantics\n",
    "   - Sparse captures keywords\n",
    "   - ColBERT captures fine-grained token matching\n",
    "   - Combining all three achieves NDCG@10 of 0.87 (IBM research)\n",
    "\n",
    "3. **Two-Stage Architecture**:\n",
    "   - Stage 1: Fast candidate retrieval (Dense + Sparse)\n",
    "   - Stage 2: Expensive reranking (ColBERT)\n",
    "   - Balances quality and performance\n",
    "\n",
    "### ðŸ’¡ Practical Insights\n",
    "\n",
    "1. **Storage Strategy**:\n",
    "   - âœ… DO: Store dense + sparse vectors (~5 KB per doc)\n",
    "   - âŒ DON'T: Store ColBERT vectors (400 KB per doc)\n",
    "   - âœ… DO: Compute ColBERT on-the-fly for reranking\n",
    "\n",
    "2. **RRF (Reciprocal Rank Fusion)**:\n",
    "   - Elegant way to combine multiple retrieval methods\n",
    "   - Formula: `score = sum(1 / (k + rank_i))`\n",
    "   - Works better than score normalization\n",
    "\n",
    "3. **ColBERT Late Interaction (MaxSim)**:\n",
    "   - For each query token, find max similarity across ALL doc tokens\n",
    "   - Average these max similarities\n",
    "   - Allows pre-computing document vectors\n",
    "\n",
    "### ðŸ“Š Performance Guidelines\n",
    "\n",
    "| Documents | Recommended Architecture | Expected Latency |\n",
    "|-----------|-------------------------|------------------|\n",
    "| < 100K | Hybrid + ColBERT | 200-300ms |\n",
    "| 100K - 1M | Hybrid only | 50-100ms |\n",
    "| > 1M | Hybrid with careful tuning | 100-200ms |\n",
    "| Real-time | Dense only | 20-30ms |\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "1. **Experiment with your own data**:\n",
    "   - Replace climate_fever with your domain\n",
    "   - Tune `candidate_limit` for quality/speed trade-off\n",
    "   - Adjust RRF parameter `k` (default: 60)\n",
    "\n",
    "2. **Evaluate retrieval quality**:\n",
    "   - Collect relevance judgments\n",
    "   - Compute NDCG@K, MRR, Recall@K\n",
    "   - A/B test different architectures\n",
    "\n",
    "3. **Optimize for production**:\n",
    "   - Use GPU for 5-10x speedup\n",
    "   - Implement caching for popular queries\n",
    "   - Monitor latency and quality metrics\n",
    "   - Set up proper error handling\n",
    "\n",
    "4. **Advanced techniques**:\n",
    "   - Add cross-encoder as final reranker\n",
    "   - Implement query expansion\n",
    "   - Use weighted RRF (adjust method importance)\n",
    "   - Experiment with other multi-vector models (e.g., ColPali)\n",
    "\n",
    "### ðŸ“š Further Reading\n",
    "\n",
    "- **BGE-M3 Paper**: https://arxiv.org/abs/2402.03216\n",
    "- **IBM Blended RAG**: https://arxiv.org/abs/2404.07220\n",
    "- **ColBERT Paper**: https://arxiv.org/abs/2004.12832\n",
    "- **Milvus Hybrid Search Docs**: https://milvus.io/docs/multi-vector-search.md\n",
    "\n",
    "---\n",
    "\n",
    "Thank you for completing this advanced tutorial! You now have the knowledge to build state-of-the-art RAG systems. Happy building! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 12: Cleanup\n",
    "\n",
    "Clean up resources when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 21\n",
    "# ============================================================\n",
    "# Cleanup Resources\n",
    "# ============================================================\n",
    "\n",
    "# Optional: Drop the collection if you want to clean up\n",
    "# Uncomment the following lines to delete the collection\n",
    "\n",
    "utility.drop_collection(collection_name)\n",
    "print(f\"ðŸ—‘ï¸  Dropped collection: {collection_name}\")\n",
    "\n",
    "# Disconnect from Milvus\n",
    "connections.disconnect(\"default\")\n",
    "print(\"âœ… Disconnected from Milvus\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cleanup complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 22\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
